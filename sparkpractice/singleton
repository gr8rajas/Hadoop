#singleton(Class with a Single Instance)

object HelloWorld {
  def main(args: Array[String]) {
    println("Hello, world!")
  }
}

#compiling:
scalac HelloWorld.scala
HelloWorld.main(Array())

#importing classes & packages
import java.util.{Date, Locale}
import java.text.DateFormat
import java.text.DateFormat._

#functions are also objects in Scala. 

#Case Classes
case classes are used to conveniently store and match on the contents of a class. You can construct them without using new.

# Word Count in Scala-Spark

val textFile = sc.textFile("hdfs://...")
val counts = textFile.flatMap(line => line.split(" "))
                 .map(word => (word, 1))
                 .reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://...")            

#Converting a given file in One format to one diff format

import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext._
object fileconv {
def main(args: Array[String]) {
val conf = new SparkConf().setAppName("Text all")   ---Name of the object should match name of the file
val sc = new SparkContext(conf)
Val textFilRDD = sc.textFile("hdfs://.....")
textFilRDD.saveAsSequenceFile("hdfs dest path")
textFilRDD.saveAsObjectFile("hdfs path")
}
}

#Reading a Sequence File
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext._
object Sequencefle {
def main(args: Array[String]) {
val conf = new Sparkconf().setAppName("SeqFile").setMaster("local[2]")
val sc = nee SparkContext(conf)
val seqfile = sc.sequenceFile("hdfs path",classOf[Text], classOf[Text]))
                . map{case (x, y) => (x.toString, y.get())}
               # . map(x => x.toString().collect().foreach(println)
}
}

#reading sequence file
object Seeqfile {
def main (args: Array[String]) {
val conf = new SparkConf().setAppName("Sequence")
val sc = new SparkContext(conf)
val seqfile = sc.sequenceFile("/user/cloudera/spark/departmentsSeq", classOf[IntWritable], classOf[Text])
             .map(rec => rec.toString()).collect().foreach(println)
}
}

#Installing Sbt and packaging

Create a File with extenion .sbt 
Enter the following info 

name := "Project name"
version := "1.0"
scalaVersion := "2.10.7"
libraryDependencies += "org.apache.spark" %% "spark-core" % "2.0.0"

# Your directory layout-----

mkdir -p ./src/main/scala/file.scala

#Packaging it
sbt package

spark-submit --class "fileconversion" --master local /home/cloudera/scala/target/scala-2.10/fileconversionproject_2.10-1.0.jar



import org.apache.hadoop.io._
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext._
import org.apache.spark.rdd.SequenceFileRDDFunctions

 object exsequencefileconversion {
      def main(args:  Array[String]) {
      val conf = new SparkConf().setAppName("Text Conv")
      val sc = new SparkContext(conf)
      val data = sc.parallelize(List(("key1", 1), ("Kay2", 2), ("Key3", 2)))
      data.saveAsSequenceFile("/cert/spark/Sequencefileoutput1")
      }
      }



#Joining Hive Tables

import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

 object joinhive {
      def main(args:  Array[String]) {
      val conf = new SparkConf().setAppName("join hive")
      val sc = new SparkContext(conf)
      val sqlContext = new HiveContext(sc)
      //val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
      sqlContext.sql("set spark.sql.shuffle.partitions=10");
      val joinData =  sqlContext.sql("select c.customer_fname,c.customer_lname,o.order_date,o.order_status from cert.orders o join cert.customers c on c.customer_id = o.order_customer_id")
      joinData.collect().foreach(println)
      }
      }


