/* Set environmental parameters for Pig script
*1.  pig.cachedbag.memusage = NNN - Set Memory Usage for all bags/relations in this script.
*                                                                                                                        Default is set to 20% (0.2) of available memory.
*2.  Can set Multi-query execution. It is turned on by default.
*    e.g  pig -M CCMAcquire.pig
*
*3. Compress the Results of Intermediate relations using LZO compression
*   set pig.tmpfilecompression = true
*   set pig.tmpfilecompression.codec = "lzo"
*
*4. Set Parallelism to specify number of reducers
*   set default parallel NN
*   else by default Pig sets the number of reducers based on the size of the input data.
*
*5. Set values for bytes processed by Reducer,
*    pig.exec.reducers.bytes.per.reducer - Defines the number of input bytes per reduce; default value is 1000*1000*1000 (1GB).
*        pig.exec.reducers.max - Defines the upper bound on the number of reducers; default is 999.
*    reducers = MIN (pig.exec.reducers.max, total input size (in bytes) / bytes per reducer)
*
*/

SET job.name 'CCM DATALOAD';
SET default_parallel '$PARALLELISM';
SET hive.metastore.uris '$METASTORE_URIS';
SET mapreduce.job.queuename '$MR_QUEUENAME';
--set pig.tmpfilecompression '$TMPFILECOMPRESSION';
--set pig.tmpfilecompression.codec 'lzo';
SET mapred.min.split.size  7584354560; --268435456;
SET mapred.max.split.size  7584354560; --268435456;
SET pig.noSplitCombination true;
set mapreduce.reduce.memory.mb 8192;
set mapreduce.reduce.java.opts -Xmx7168m;
set mapreduce.map.memory.mb 8192;
set mapreduce.map.java.opts -Xmx7168m;



####Loader Input Format Class
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.JobContext;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;

public class LoaderInputFormat extends FileInputFormat<LongWritable, Text>
{
  public RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context)
  {
    String delimiter = "\002";
    byte[] recordDelimiterBytes = null;
    if (delimiter != null)
      recordDelimiterBytes = delimiter.getBytes();
    return new LineRecordReader(recordDelimiterBytes);
  }

  protected boolean isSplitable(JobContext context, Path file)
  {
    CompressionCodec codec = 
      new CompressionCodecFactory(context.getConfiguration()).getCodec(file);
    return codec == null;
  }
}

####New Pig Storage

import org.apache.hadoop.mapreduce.InputFormat;
import org.apache.pig.builtin.PigStorage;

public class NewPigStorage extends PigStorage
{
  public NewPigStorage(String delimiter)
  {
    super(delimiter);
  }

  public NewPigStorage()
  {
  }

  public InputFormat getInputFormat() {
    return new LoaderInputFormat();
  }
}



####Null to Space

import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;

public class ConvertNullToSpace extends EvalFunc<String>
{
  private static int numOfArguments;

  public String exec(Tuple input)
    throws IOException
  {
    try
    {
      numOfArguments = input.size();
      String columnValue = (String)input.get(0);
      if (numOfArguments == 1) {
        if (columnValue == null) {
          return " ";
        }
        return columnValue;
      }
      return columnValue;
    } catch (Exception e) {
      throw new IOException("Caught exception processing input row" + e);
    }
  }
}
